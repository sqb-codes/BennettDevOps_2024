1. Create node project
  -> npm init -y

2. Build Image using docker
  -> docker build .
  -> docker build -t node-app .
  -t stands for tag

3. Show process
  -> docker ps
    - show running process
  -> docker ps -a
    - show all process

4. Show images
  -> docker image ls

5. Run container
  -> docker run -p 3000:3000 --name node-app node-app-img
  -> docker run -d -p 3000:3000 --name node-app node-app-img
  -d stands for detach mode. Release the terminal after running container

6. Remove container
  -> docker rm node-app
  -> docker rm node-app -f
  -f stands for force

7. Show docker working directory
  -> docker exec -it node-app bash

8. Bind Mount
  
  For windows command prompt
  -> docker run -d -v %cd%:/app -p 3000:3000 --name node-app node-app-img

  For windows powershell / mac / linux
  -> docker run -d -v ${pwd}:/app -p 3000:3000 --name node-app node-app-img

- If we delete folder like node_modules from our directory
then it will delete also from /app

Anonymous volumes

-> docker run -d -v ${pwd}:/app -v /app/node_modules -p 3000:3000 --name node-app node-app-img

- Now we want to make read only volumes

Read only Mount volumes
-> docker run -d -v ${pwd}:/app:ro -v /app/node_modules -p 3000:3000 --name node-app node-app-img

Remove volume + container
-> docker rm node-app -fv

Passing envrionment variable

-> docker run -d -v ${pwd}:/app:ro -v /app/node_modules --env PORT=4000 -p 3000:4000 --name node-app node-app-img

Use .env file instead of passing envrionment variable in command

-> docker run -d -v ${pwd}:/app:ro -v /app/node_modules --env-file ./.env -p 3000:4000 --name node-app node-app-img


Run Docker Compose
- docker-compose up -d

Remove Docker Compose
- docker-compose down
- docker-compose down -v

Forcefully creates new image
- docker-compose up -d --build

Run dev and prod commands
Dev command : docker-compose -f docker-compose.yml -f docker-compose.dev.yml up -d
Prod command : docker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d

Dev command : docker-compose -f docker-compose.yml -f docker-compose.dev.yml up -d --build

Remove docker compose container
Dev command : docker-compose -f docker-compose.yml -f docker-compose.dev.yml down -v


docker exec -it mongoContainerName bash
- it will start mongo container 

cd bin
mongosh -u "root" -p "root"

Shortcut to enter in mongo shell
- docker exec -it mongoContainerName /bin/mongosh -u "root" -p "root"


Now if we delete container then our database will also be deleted
So the solution is to persist data using volumes

Now we won't use -v flag while deleting container. Because we don't want to delete volumes that contains database

Updated command to Remove docker compose container
: docker-compose -f docker-compose.yml -f docker-compose.dev.yml down


Install ping in bash
- apt-get update -y
- apt-get install -y iputils-ping

Now Build CRUD operations and test them using Postman

Now we are going to add nginx server

Now after adding default.conf, we will scale up node-app instances

server {
    listen 80;

    // location block is used to configure how nginx should handle request to the root of the server
    location /api {

      // X-Real-IP - header is set to the IP address of the client making the request $remote_addr
      // X-Forwarded-For - header is set to the original client IP address...especially useful when request passes through proxies
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;

        // Host - set Host header value of $http_post variable and add an additional header X-NginX-Proxy
        // Host - header specifies the hostname of server
        proxy_set_header Host $http_host;
        proxy_set_header X-NginX-Proxy true;

        // this specifies the backend server to which nginx should proxy request
        proxy_pass http://node-app:3000;

        // disables automatic redirection of the proxy_pass, because by default nginx rewrites the location and refresh response headers returned by the proxied server...
        proxy_redirect off;

    }
}

It will create 2 instances of node-app
docker-compose -f docker-compose.yml -f docker-compose.dev.yml up -d --scale node-app=2


How production will work ?
Phase 1
1. Create EC2 instance
2. Install docker and docker-compose
3. Set envrionment variables on EC2
4. Push code on git
5. Pull code from git on EC2
6. Run + Build Docker Compose

Phase 2
7. Introduce Docker Hub and push image on docker hub
8. Pull image on EC2

Phase 3
9. Introduce Watchtower - Automate the pull operations on EC2

Phase 4 - Sonarqube + Trivy

Phase 5 - Pipeline using Jenkins


==========================
EC2 Commands
sudo apt-get update

Install Docker
sudo apt-get install docker.io -y
sudo usermod -aG docker $USER
newgrp docker
sudo chmod 777 /var/run/docker.sock

Install Docker-Compose
sudo apt-get install docker-compose

Push Code on Git
git init
git add .
git commit -m "crud app"
git branch -M main
git remote add origin URL
git push -u origin main


Setup envrionment variables on EC2
Command : vi .env
- it will create .env file to store our envrionment variables

NODE_ENV=production
MONGO_USER=root
MONGO_PASSWORD=root
MONGO_INITDB_ROOT_USERNAME=root
MONGO_INITDB_ROOT_PASSWORD=root

Now open .profile and add this command in last
Command : vi .profile
set -o allexport; source /root/.env; set +o allexport


Otherwise export one by one
export NODE_ENV=production
export MONGO_USER=root
export MONGO_PASSWORD=root
export MONGO_INITDB_ROOT_USERNAME=root
export MONGO_INITDB_ROOT_PASSWORD=root



Now run docker prod on EC2
docker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d

Copy IP of EC2 and replace localhost in Postman

===== End of Phase-1 =====


Phase 2
Introduce Docker Hub and push image on docker hub

Command - docker image tag 03-expresscrud_docker-node-app ravisqb/crud-docker
Command - docker push ravisqb/crud-docker

Change image name in docker-compose.yml
  node-app:
    build: .
    image: ravisqb/crud-docker

Now pull docker image on EC2
Command - docker-compose -f docker-compose.yml -f docker-compose.prod.yml pull

===== End of Phase-2 =====

Phase 3
Introduce Watchtower - Automate the pull operations on EC2

docker run -d --name watchtower -e WATCHTOWER_TRACE=true -e WATCHTOWER_DEBUG=true -e WATCHTOWER_POLL_INTERVAL=20 -v /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower


===== End of Phase-3 =====


Phase 4 - Sonarqube + Trivy
1. Enable ports for Jenkins, Sonarqube

Sonarqube
- tool used for continuous inspecting the code quality
- used to inspect security of code

Install Sonarqube

docker run -d --name sonar -p 9000:9000 sonarqube:lts-community



Now Setup Trivy
Trivy is an open-source vulnerability scanner for containers and other artifacts in the DevOps pipeline. It is designed to help developers and security professionals identify vulnerabilities and security issues in container images, packages, and filesystems.

To install Trivy:
sudo apt-get install wget apt-transport-https gnupg lsb-release
wget -qO - https://aquasecurity.github.io/trivy-repo/deb/public.key | sudo apt-key add -
echo deb https://aquasecurity.github.io/trivy-repo/deb $(lsb_release -sc) main | sudo tee -a /etc/apt/sources.list.d/trivy.list
sudo apt-get update
sudo apt-get install trivy        

to scan image using trivy
Scan the current file system - trivy fs .
Scan the container image - trivy image <imageid>

=====End of Phase 4=====

Phase 5 - Pipeline using Jenkins
- pre-requisite to install jenkins
  - install java

Commands to install Java

sudo apt update
sudo apt install openjdk-17-jre
java -version

Install Jenkins
sudo wget -O /usr/share/keyrings/jenkins-keyring.asc  https://pkg.jenkins.io/debian-stable/jenkins.io-2023.key

echo deb [signed-by=/usr/share/keyrings/jenkins-keyring.asc] \
  https://pkg.jenkins.io/debian-stable binary/ | sudo tee \
  /etc/apt/sources.list.d/jenkins.list > /dev/null

sudo apt-get update

sudo apt-get install jenkins

sudo systemctl start jenkins.service

sudo systemctl status jenkins

Once jenkins is started : copy public IP and paste in browser
PublicIP:8080

Copy path from jenkins and go back to EC2
EC2 - sudo cat <path>
Copy and paste password in jenkins


Install Necessary Plugins in Jenkins:
Goto Manage Jenkins →Plugins → Available Plugins →

Install below plugins
1 Eclipse Temurin Installer (Install without restart)
2 SonarQube Scanner (Install without restart)
3 NodeJs Plugin (Install Without restart)
4 Email Extension Plugin


Configure Java and Nodejs in Global Tool Configuration

Goto Manage Jenkins → Tools → Install JDK(17) and NodeJs(22)→ Click on Apply and Save

SonarQube with Jenkins

Open SonarQube
Administration -> Security -> Users -> Update Token
Copy token and go back to Jenkins

Create the token
Goto Jenkins Dashboard → Manage Jenkins → Credentials → Add Credentials -> Select Secret Text -> Paste token
ID - Sonar-token
Description - Sonar token

After adding sonar token Click on Create

Once done you'll be able to see Sonar-token in Global Credentials

We will install a sonar scanner in the tools.
Manage genkins -> System -> Search SonarQube
Name - sonar-server
URL - EC2 IP Address with PORT 9000
Server authentication token - Sonar-token

Apply and Save

Install Sonar Scanner
Go to Manage Jenkins -> Tools -> Sonarqube Scanner
Name - sonar-scanner
Leave version as it is

Apply and Save


Configure CI/CD Pipeline in Jenkins:

To Create pipeline
Click on New Item -> Give Name -> Select pipeline -> Ok
Now go to pipeline section and  Put the pipeline script and apply and save

Click Build Now to run the pipeline

Jenkins Pipeline
pipeline {
    agent any

    tools {
        jdk 'jdk17'
        nodejs 'node22'
    }
    environment {
        SCANNER_HOME=tool 'sonar-scanner'
    }
    stages {
        stage('clean workspace'){
            steps {
                cleanWs()
            }    
        }
        stage('Checkout from GIT') {
            steps {
                git branch: 'main', url: 'https://github.com/sqb-codes/crud-app.git'
            }
        }
        stage('Sonarqube Analysis') {
            steps {
                withSonarQubeEnv('sonar-server') {
                    sh "$SCANNER_HOME/bin/sonar-scanner -Dsonar.projectName=CRUD -Dsonar.projectKey=CRUD"
                }
            }
        }
        stage('quality gate') {
            steps {
                script {
                    waitForQualityGate abortPipeline: false, credentialsId: 'SonarScanner'
                }
            }
        }
        stage('Install Dependencies') {
            steps {
                sh "npm install"
            }
        }
    }

}


Install Dependency-Check and Docker Tools in Jenkins

Install Dependency-Check Plugin:
Go to "Dashboard" in your Jenkins web interface.

Navigate to "Manage Jenkins" → "Manage Plugins."
Click on the "Available" tab and search for "OWASP Dependency-Check."
Check the checkbox for "OWASP Dependency-Check" and click on the "Install without restart" button.

Configure Dependency-Check Tool:
After installing the Dependency-Check plugin, you need to configure the tool.
Go to "Dashboard" → "Manage Jenkins" → "Global Tool Configuration."
Find the section for "OWASP Dependency-Check."
Add the tool's name, e.g., "DP-Check."
Save your settings.

Install Docker Tools and Docker Plugins:
Go to "Dashboard" in your Jenkins web interface.
Navigate to "Manage Jenkins" → "Manage Plugins."
Click on the "Available" tab and search for "Docker."

Check the following Docker-related plugins:
Docker
Docker Commons
Docker Pipeline
Docker API
docker-build-step
Click on the "Install without restart" button to install these plugins.

Add DockerHub Credentials:
To securely handle DockerHub credentials in your Jenkins pipeline, follow these steps:
Go to "Dashboard" → "Manage Jenkins" → "Manage Credentials."
Click on "System" and then "Global credentials (unrestricted)."
Click on "Add Credentials" on the left side.
Choose "Secret text" as the kind of credentials.

Enter your DockerHub credentials (Username and Password) and give the credentials an ID (e.g., "docker").
Click "OK" to save your DockerHub credentials.
