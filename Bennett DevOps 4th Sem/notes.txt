1. Create node project
  -> npm init -y

2. Build Image using docker
  -> docker build .
  -> docker build -t node-app .
  -t stands for tag

3. Show process
  -> docker ps
    - show running process
  -> docker ps -a
    - show all process

4. Show images
  -> docker image ls

5. Run container
  -> docker run -p 3000:3000 --name node-app node-app-img
  -> docker run -d -p 3000:3000 --name node-app node-app-img
  -d stands for detach mode. Release the terminal after running container

6. Remove container
  -> docker rm node-app
  -> docker rm node-app -f
  -f stands for force

7. Show docker working directory
  -> docker exec -it node-app bash

8. Bind Mount
  
  For windows command prompt
  -> docker run -d -v %cd%:/app -p 3000:3000 --name node-app node-app-img

  For windows powershell / mac / linux
  -> docker run -d -v ${pwd}:/app -p 3000:3000 --name node-app node-app-img

- If we delete folder like node_modules from our directory
then it will delete also from /app

Anonymous volumes

-> docker run -d -v ${pwd}:/app -v /app/node_modules -p 3000:3000 --name node-app node-app-img

- Now we want to make read only volumes

Read only Mount volumes
-> docker run -d -v ${pwd}:/app:ro -v /app/node_modules -p 3000:3000 --name node-app node-app-img

Remove volume + container
-> docker rm node-app -fv

Passing envrionment variable

-> docker run -d -v ${pwd}:/app:ro -v /app/node_modules --env PORT=4000 -p 3000:4000 --name node-app node-app-img

Use .env file instead of passing envrionment variable in command

-> docker run -d -v ${pwd}:/app:ro -v /app/node_modules --env-file ./.env -p 3000:4000 --name node-app node-app-img


Run Docker Compose
- docker-compose up -d

Remove Docker Compose
- docker-compose down
- docker-compose down -v

Forcefully creates new image
- docker-compose up -d --build

Run dev and prod commands
Dev command : docker-compose -f docker-compose.yml -f docker-compose.dev.yml up -d
Prod command : docker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d

Dev command : docker-compose -f docker-compose.yml -f docker-compose.dev.yml up -d --build

Remove docker compose container
Dev command : docker-compose -f docker-compose.yml -f docker-compose.dev.yml down -v


docker exec -it mongoContainerName bash
- it will start mongo container 

cd bin
mongosh -u "root" -p "root"

Shortcut to enter in mongo shell
- docker exec -it mongoContainerName /bin/mongosh -u "root" -p "root"


Now if we delete container then our database will also be deleted
So the solution is to persist data using volumes

Now we won't use -v flag while deleting container. Because we don't want to delete volumes that contains database

Updated command to Remove docker compose container
: docker-compose -f docker-compose.yml -f docker-compose.dev.yml down


Install ping in bash
- apt-get update -y
- apt-get install -y iputils-ping

Now Build CRUD operations and test them using Postman

Now we are going to add nginx server

Now after adding default.conf, we will scale up node-app instances

server {
    listen 80;

    // location block is used to configure how nginx should handle request to the root of the server
    location /api {

      // X-Real-IP - header is set to the IP address of the client making the request $remote_addr
      // X-Forwarded-For - header is set to the original client IP address...especially useful when request passes through proxies
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;

        // Host - set Host header value of $http_post variable and add an additional header X-NginX-Proxy
        // Host - header specifies the hostname of server
        proxy_set_header Host $http_host;
        proxy_set_header X-NginX-Proxy true;

        // this specifies the backend server to which nginx should proxy request
        proxy_pass http://node-app:3000;

        // disables automatic redirection of the proxy_pass, because by default nginx rewrites the location and refresh response headers returned by the proxied server...
        proxy_redirect off;

    }
}

It will create 2 instances of node-app
docker-compose -f docker-compose.yml -f docker-compose.dev.yml up -d --scale node-app=2


How production will work ?
Phase 1
1. Create EC2 instance
2. Install docker and docker-compose
3. Set envrionment variables on EC2
4. Push code on git
5. Pull code from git on EC2
6. Run + Build Docker Compose

Phase 2
7. Introduce Docker Hub and push image on docker hub
8. Pull image on EC2

Phase 3
9. Introduce Watchtower - Automate the pull operations on EC2

Phase 4 - Sonarqube + Trivy

Phase 5 - Pipeline using Jenkins


==========================
EC2 Commands
sudo apt-get update

Install Docker
sudo apt-get install docker.io -y
sudo usermod -aG docker $USER
newgrp docker
sudo chmod 777 /var/run/docker.sock

Install Docker-Compose
sudo apt-get install docker-compose

Push Code on Git
git init
git add .
git commit -m "crud app"
git branch -M main
git remote add origin URL
git push -u origin main


Setup envrionment variables on EC2
Command : vi .env
- it will create .env file to store our envrionment variables

NODE_ENV=production
MONGO_USER=root
MONGO_PASSWORD=root
MONGO_INITDB_ROOT_USERNAME=root
MONGO_INITDB_ROOT_PASSWORD=root

Now open .profile and add this command in last
Command : vi .profile
set -o allexport; source /root/.env; set +o allexport

Now run docker prod on EC2
docker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d

Copy IP of EC2 and replace localhost in Postman

===== End of Phase-1 =====

